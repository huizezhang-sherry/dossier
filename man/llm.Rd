% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llm.R
\name{extract_decisions}
\alias{extract_decisions}
\title{Summarize PDF via LLM}
\usage{
extract_decisions(prompt_file, pdf, file = NULL, ..., llm_model = "claude")
}
\arguments{
\item{prompt_file}{A single string containing the prompt}

\item{pdf}{A single string specifying the path to a PDF file}

\item{file}{A single string specifying where to save the output. By default,
it saves a markdown file with the same name as the PDF file in the same directory.}

\item{...}{other arguments supplied to the LLM chat function, such as `seed`, `temperature`, etc.}

\item{llm_model}{One of `"claude"` or `"gemini"` for processing pdf documents. Default to `"claude"`}
}
\value{
Writes output to the specified file.
}
\description{
The output from LLM may contains additional notes about the process,
unexpected line breaks, etc. We recommend capturing the output in a markdown
file and instruct the LLM to write the output in a json block. Use the function
[clean_md] to extract the decisions from the JSON block.
}
\seealso{
[clean_md]
}
